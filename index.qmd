---
title: Paper Review
toc: false
authors:
  - name: Shauna Heron
    affiliation: Laurentian University
    roles: writing
    corresponding: true
bibliography: references.bib
---

# **Introduction and Motivation**

This review analyzes a study by @garriga2023, published in *Cell Medical*, titled *"Combining Clinical Notes with Structured Electronic Health Records Enhances the Prediction of Mental Health Crises."*

The research investigates the utility of combining unstructured clinical notes with structured data from electronic health records (EHR) to improve the prediction of mental health crises [@garriga2023]. The relevance of this research is underscored by a global rise in mental health-related hospitalizations coinciding with significant workforce challenges, stressing the need for tools that might anticipate demand which is necessary to manage caseloads more efficiently. Even in Ontario, recent data highlights that hospitalizations for mental health conditions, particularly among youth, have surged since the COVID-19 pandemic [@addressi2022]. Moreoever, those requiring hospitalization (e.g., emotional breakdowns, substance overdoses and suicide attempts) have increased by 90% [@addressi2022]--many of which many might have been prevented with early intervention, suggesting that an automated tool for predicting crisis onset might help anticipate and address mental health crises before they peak.

As the authors point out, leveraging EHRs to bolster clinical decision-making is not new [@garriga2023]. Clinicians and researchers have long utilized structured data like diagnosis codes, lab results, and medication records to inform predictive models. However, computational limits as well as a lack of sophisticated modeling tools necessary to implement such models mean that unstructured data like clinical notes and other free-form text, are most often left out of feature sets, despite the rich contextual information that clinical notes contain.

# **Hybrid Solution**

With this gap in mind, the authors proposed a hybrid solution that built on their previous study [@garriga2022] that leveraged structured EHRs to predict crisis events with a weekly crisis-risk score. In the current study, they included unstructured data in the form of clinical notes to bolster those predictions. Specifically, the author's hypothesized that models that combined structured and unstructured data (such as clinical notes) would predict the probability of a weekly crisis event more accurately than a model including just structured data alone.

Structured data features included static features like demographic information (e.g., postal code, race, gender), features that described the patients most recent interaction with the hospital (e.g., substance use identified, new medication), and lag-features that quantified the elapsed time since a specific event (e.g., time since the last crisis episode). Unstructured data was made up of free-form clinical notes written by clinicians. From these notes, semantic features were created using a BERT model, which is a pre-trained model good at extracting enhanced language features by considering the words that come before and after each word in a sentence [@bert101].

Leveraging those features, four models were trained to predict the risk of relapse within a 28-day window as a binary classification (relapse versus no relapse): i) a deep neural network trained on structured data (referred to as Struct DNN), ii) a model trained exclusively on unstructured data (Text DNN), iii) a hybrid model that integrated both structured and unstructured data (Hybrid DNN) and importantly, iv) an ensemble of models that utilized predictions from the Hybrid DNN when unstructured data was available and predictions from the Struct DNN model otherwise (referred to as Ensemble DNN). See Figure 1 below for a flow chart of the experimental and modeling process.

**Model Architecture and Data Flow Diagram:**

![Figure: Model Architecture](images/paste-1.png){alt="Figure: Model Architecture"} Figure 1. Overview of the Hybrid DNN model and data input structure, showing how structured and unstructured data are processed. Adapted from [@garriga2023, pg. 3]

To address the issue of class imbalance (since crisis relapses are rare with only 1.3% prevalence), models were tuned to maximize the area under the precision-recall curve (AUPRC)--which as we learned in class is a particularly useful metric when evaluating results produced by imbalanced classes. Moreoever, as highlighted in our lessons, precision-recall metrics provide a better measure of a model's predictive quality than accuracy in healthcare settings where false positives can be just as costly as false negatives [@mc2024; @garriga2023].

To evaluate model performance against non-machine learning methods used to evaluate crisis-risk, two baselines were built: a 5-factor logistic regression model informed by significant variables suggested in prior literature (LogReg5) and a heuristic model that ranked patients by the total number of crises they had experienced in the past year [@garriga2023].

# **Results**

The experiments revealed that the best performing model trained on structured data alone was an XGBoost classifier, a tree-based model implementing gradient boosting. The best performing model on unstructured data alone as well as combined structured and unstructured data was a feed-forward deep neural network (DNN). The best performing model overall was the ensemble DNN that took a flexible approach, utilizing predictions from a structured DNN when unstructured data was unavailable and the hybrid DNN combining data types when it was enabling the model to draw insights from semantic features when available to increase overall performance.

Interestingly, the authors also examined the impact that various features in the models had on predictive performance with Shapley additive explanations (SHAP). Their use of SHAP to assess the predictive power of different types of data as well as different features is significant because it increases model interpretability and transparency which is important in healthcare settings were decisions need to be trusted by front line staff. Though an in depth discussion of SHAP analysis is beyond the scope of this review, it is worth noting that the authors implemented it; allowing for both cohort-level and instance-specific explanations for predictions [@welcome].

## **Performance Comparison Across Models:**

The comparative performance of the Structured-XGBoost, unstructured DNN and the hybrid DNN combining data types at 10-week intervals across the 52 weeks are illustrated in the figure below. The increase in area under the precision-recall curves (AUPRC) over time suggests an increase in performance across models at each 10-week interval [@measurin2019].

According to the authors, the increase in AUPRC over time is only weakly related to an increase in EHR data over time, and more to the clinical fact that the chance of a crisis-relapse increases the longer a patient is in treatment. The structured XGBoost model outperformed both models at 10-weeks, but at 20 weeks, when clinical note data becomes available, the XGBoost model is surpassed by the hybrid model. Higher AUPRC values indicate better precision in predicting true crisis-events relative to non-crisis events that are incorrectly predicted.

![Figure: Precision-Recall Curves](images/paste-3.png)

*Figure 3. Precision-recall curves for the Hybrid DNN, Struct DNN, and Text DNN, across 10-week intervals illustrating an increase in performance of all models over time as the amount of information increases across weeks. Adapted from [@garriga2023, pg. 5]*

The receiver operating characteristic (AUROC) comparisons below, similarly reflect the model's ability to distinguish between true positive and negative predictions, however AUROC is considered a more forgiving metric when evaluating imbalanced datasets because it measures the model's overall capacity to separate classes, rather than focusing exclusively on predicting positive classes (crisis events). As indicated in Figure 4, the ability of both the structured XGBoost and unstructured DNN models to distinguish between classes *decrease* over time until 40 weeks when both the structured and hybrid model increase in performance. Interestingly, the performance of the unstructured-only model decreases across the entire 52 weeks, which the authors suggest highlights the complexity of modelling mental health crises: while the quantity of data in a given client file tends to increase over time, the overall number of patients requiring treatment *decreases* which impacts performance of the neural network (i.e., text input tends to require more examples or training instances to perform adequately in NNs) [@garriga2023]. Note too how the structured-only xgboost is best at predicting crisis with fewer than 10% of notes, but with 20% of weeks with available notes, the hybrid model's performance offers a small improvement on performance. This indicates that the addition of unstructured data helped the model make more accurate predictions across a range of thresholds, improving its overall discriminatory power.

**AUROC Across 52 weeks:**

![Figure: Model Accuracy](images/paste-4.png) *Figure 4. Performance of the Hybrid model compared to the unstructured and structured only models in predicting crisis episodes at 10 week intervals. Points and lines indicate mean and ± standard deviation values computed in the 52 weeks of the test set. Adapted from [@garriga2023, pg. 5]*

# **Critical Analysis**

The paper offered a compelling case for the inclusion of unstructured data to bolster structured data in predictive modeling of healthcare data, highlight several important techniques including the use of an ensemble of models depending on the data available as well as demonstrating the importance of feature analysis which allows both cohort-level and instance-specific explanations which is important for enhancing transparency in healthcare machine learning models. There were several areas that to keep in mind. which supported their hypothesis that incorporating clinical notes would improve the model's ability to identify potential mental health crises earlier compared to models trained on structured data alone.

While the best model achieved notable AUPRC and AUROC scores in the context of predicting a rare event like a mental health crises, the results still reflect a gap between predictive performance and the reliability needed for clinical decision-making. The scores tells us that while the model can identify crises better than random guessing, it may still flag many non-crisis events potentially overburdening clinicians. It would have been interesting to discuss what a threshold for clinical utility would be in any implementations. Nevertheless, the authors did point out difficulties inherent in utilizing clinical data to predict rare events–as the probability of an event decreases over time within a cohort, the number of people in that cohort will decrease.

SImilarly, though the author's conducted a robust feature analysis which is important for enhancing transparency in healthcare machine learning models, the contribution of specific unstructured features remained somewhat opaque. Nevertheless, ....

Moreoever, the model was trained on a single healthcare setting's data, which means there could be challenging generalizing to another setting with different populations or documentation practices. Future research could explore the generalizability of these findings to other domains within healthcare. In particular, in my own research I am wondering how this method could be used to predict case complexity to caseload management. and refine @garriga2023 's approach to make the integration of unstructured data more interpretable. Another potential direction was improving methods for natural language processing (NLP) to better extract meaningful insights from clinical notes, which could further enhance predictive performance.

I especially liked their inclusion of SHAP values and analyses of variables that contributed most to model predictions--TALK MORE ABOUT THIS.

Secondly, the study primarily focused on a single health system, which might limit the generalizability of the results. Future studies should consider multi-site evaluations to determine if the predictive power of these models held in different settings with varied data quality and clinical practices.

Lastly, while the results were promising, they relied heavily on high-quality, well-documented clinical notes. In practice, the quality of clinical notes could vary significantly, and the models' reliance on unstructured data could lead to inconsistencies in predictions if the notes were incomplete or ambiguous.

# **Conclusion**

The study effectively demonstrated that incorporating unstructured data, such as clinical notes, alongside structured EHR data improved the ability to predict mental health crises. Importantly their findings underscore the potential for including qualitative clinical information in predictive modeling and highlights a novel ensemble methodology that can be used to build a flexible, combined model that can extract value from both data types. depening on what is available in the client record.